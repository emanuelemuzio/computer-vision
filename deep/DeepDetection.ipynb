{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Riconoscimento tramite Deep Learning\n",
    "\n",
    "Requisiti:\n",
    "- Rifacimento assignment sul riconoscimento, questa volta utilizzando però una rete neurale profonda.\n",
    "\n",
    "Per questa esercitazioni, le scelte tecniche sono state:\n",
    "- Yolov8 per la face detection nelle immagini e video\n",
    "- Classificazione tramite CNN profonda in Tensorflow\n",
    "\n",
    "Bonus:\n",
    "- Utilizzo di una label extra per i volti sconosciuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\emanu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import cv2 as cv\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Rescaling, Resizing, RandomFlip, RandomRotation, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo step consiste nel far scorrere il nostro face detector lungo tutte le immagini già acquisite, farne il rescale per renderle quanto più simili possibili alle immagini senza una identità precisa (che da ora in avanti definiremo Unknown) e fare data augmentation per aumentare i nostri dati a disposizione.\n",
    "\n",
    "Per la data augmentation, in particolare, sono state applicate trasformazioni randomizzate:\n",
    "- flip orizzontale dell'immagine\n",
    "- leggera rotazione (fattore 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializziamo Yolov8 e le immagini per fare training, test e validazione\n",
    "\n",
    "train = None\n",
    "test = None\n",
    "val = None\n",
    "yolo = YOLO('yolov8n-face.pt')\n",
    "\n",
    "Imgs_train = Imgs_test = Imgs_val = None\n",
    "Labels_train = Labels_test = Labels_val = None\n",
    "\n",
    "if os.path.exists('train.pkl'):\n",
    "    trainfile = open('train.pkl', 'rb')\n",
    "    testfile = open('test.pkl', 'rb')\n",
    "    valfile = open('val.pkl', 'rb')\n",
    "\n",
    "    train = pickle.load(trainfile) \n",
    "    test = pickle.load(testfile)\n",
    "    val = pickle.load(valfile)\n",
    "\n",
    "    Imgs_train = train['imgs']\n",
    "    Imgs_test = test['imgs']\n",
    "    Imgs_val = val['imgs']\n",
    "\n",
    "    Labels_train = train['labels']\n",
    "    Labels_test = test['labels']\n",
    "    Labels_val = val['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per le dimensioni di train-test-validation, abbiamo suddiviso le immagini etichettate precedentemente ottenute con le seguenti percentuali, rispettivamente 60-20-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\emanu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Processo di face detection dal nostro dataset di circa 30k foto (compresi volti etichettati come Unknown, non facenti parte del nostro pool di 4 identità note)\n",
    "\n",
    "resize_and_rescale = Sequential([\n",
    "        Resizing(64, 64),\n",
    "        Rescaling(1./255)\n",
    "    ])\n",
    "\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "if not os.path.exists('train.pkl'):\n",
    "    root = 'train'\n",
    "    paths = []\n",
    "    labels = []\n",
    "\n",
    "    for dir in os.listdir(root):\n",
    "        tmp = glob.glob(f'{root}/{dir}/*.jpg')\n",
    "        labels.extend(len(tmp) * [dir])\n",
    "        paths.extend(tmp)\n",
    "\n",
    "    imgs = []\n",
    "    img_labels = []\n",
    "\n",
    "    # Sfruttiamo la funzionalità di yolo che ci permette di andare a fare \n",
    "    # face detection considerando solo le bbox con label 0, cioè con classe 'person'.\n",
    "    # Andiamo inoltre a prendere soltanto la bbox la cui label ha la confidenza più alta;\n",
    "    # Il motivo di ciò è che durante i test è emerso che anche i volti riflessi in eventuali\n",
    "    # finestre venivano rilevati, dando vita a una sorta di falso positivo (ma con comunque)\n",
    "    # una confidenza minore rispetto al volto \"reale\". Facciamo infine il crop dell'immagine\n",
    "    # per andare a estrarre solo la porzione desiderata, usando due piccoli modelli per \n",
    "    # fare resize e rescale delle immagini per normalizzarle e per fare data augmentation,\n",
    "    # applicando una piccola rotazione casuale e un flip orizzontale randomico\n",
    "\n",
    "    for i in range(len(paths)):\n",
    "        path = paths[i]\n",
    "        img = cv.imread(path)\n",
    "        result = yolo(img, classes=0, verbose=False)[0].numpy()\n",
    "        if len(result) != 0:\n",
    "            boxes = result.boxes\n",
    "            conf = boxes.conf\n",
    "            argmax = np.argmax(conf)\n",
    "            box = boxes[argmax]\n",
    "            xyxy = box.xyxy[0]\n",
    "            xyxy = [round(xy) for xy in xyxy]\n",
    "            crop = img[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]\n",
    "\n",
    "            crop = resize_and_rescale(crop).numpy()\n",
    "            crop_aug = data_augmentation(crop).numpy()\n",
    "\n",
    "            imgs.append(crop)\n",
    "            imgs.append(crop_aug)\n",
    "            img_labels.append(labels[i])\n",
    "            img_labels.append(labels[i])\n",
    "\n",
    "    # Procediamo a dividere le nostre immagini etichettate in 3 insiemi per fare poi\n",
    "    # train, test e validation. Le dimensioni di questi saranno rispettivamente il\n",
    "    # 60%, 20% e 20% del totale delle immagini a disposizione.\n",
    "    # Salviamo infine tramite pickle il lavoro svolto per evitare sprechi di tempo futuri\n",
    "\n",
    "    Imgs_train, Imgs_test, Labels_train, Labels_test = train_test_split(imgs, img_labels, test_size=0.4)\n",
    "    Imgs_val, Imgs_test, Labels_val, Labels_test = train_test_split(Imgs_test, Labels_test, test_size=0.5)\n",
    "    \n",
    "    Imgs_train = np.asarray(Imgs_train)\n",
    "    Imgs_test = np.asarray(Imgs_test)\n",
    "    Imgs_val = np.asarray(Imgs_val)\n",
    "    \n",
    "    Labels_train = LabelEncoder().fit_transform(Labels_train)\n",
    "    Labels_test = LabelEncoder().fit_transform(Labels_test)\n",
    "    Labels_val = LabelEncoder().fit_transform(Labels_val)\n",
    "\n",
    "    trainfile = open('train.pkl', 'wb')\n",
    "    testfile = open('test.pkl', 'wb')\n",
    "    valfile = open('val.pkl', 'wb')\n",
    "\n",
    "    train = {\n",
    "        \"imgs\" : Imgs_train,\n",
    "        \"labels\" : Labels_train\n",
    "    }\n",
    "\n",
    "    test = {\n",
    "        \"imgs\" : Imgs_test,\n",
    "        \"labels\" : Labels_test\n",
    "    }\n",
    "\n",
    "    val = {\n",
    "        \"imgs\" : Imgs_val,\n",
    "        \"labels\" : Labels_val\n",
    "    }\n",
    "\n",
    "    pickle.dump(train, trainfile)\n",
    "    pickle.dump(test, testfile)\n",
    "    pickle.dump(val, valfile)\n",
    "\n",
    "    trainfile.close()\n",
    "    testfile.close()\n",
    "    valfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgShape = (64, 64, 3)\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sono stati effettuati diversi tentativi al fine di scegliere il modello migliore per il nostro compito, variando sia iperparametri che struttura della rete, avvalendosi anche in alcuni tentativi di regolarizzazione l2 dei pesi in quanto, durante le prime prove, il problema più evidente era un overfitting in fase di test, che si traduceva poi in accuracy estremamente basse in fase di test.\n",
    "\n",
    "Per evitare sprechi di tempo e risorse, è stato utile aggiungere alla rete un meccanismo di early stopping mediamente sensibile sull'accuracy, nel caso in cui si arrivasse ad una situazione piatta e non riuscisse più ad aumentare dopo alcune epoche. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\emanu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\emanu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'muzio_2.keras'\n",
    "\n",
    "if os.path.exists(model_name):\n",
    "  model = load_model(model_name)\n",
    "else:\n",
    "\n",
    "  model = Sequential([\n",
    "    Conv2D(64, 3, activation='relu', padding='same', input_shape=imgShape),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, 3, activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(256, 3, activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "  early_stop = EarlyStopping(monitor='accuracy', patience = 3, restore_best_weights = True)\n",
    "\n",
    "  hist = model.fit(Imgs_train, \n",
    "                  Labels_train, \n",
    "                  epochs=100,\n",
    "                  batch_size=64,\n",
    "                  validation_data=(Imgs_val, Labels_val),\n",
    "                  callbacks=[early_stop]\n",
    "                  )\n",
    "  \n",
    "  model.save('muzio.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraiamo 20 frame e annotiamoli manualmente per fare model selection: questi 20 frame non andranno a coincidere ovviamente con la porzione del video usata per i test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = ['Davide', 'Francesco', 'Gabriele', 'Stefano', 'Unknown']\n",
    "\n",
    "annotation = None\n",
    "font = cv.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "if os.path.exists('annotateset.pkl'):\n",
    "    annotationfile = open('annotateset.pkl', 'rb')\n",
    "    annotation = pickle.load(annotationfile)\n",
    "    annotationfile.close()\n",
    "else:\n",
    "    annotation = []\n",
    "    cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "    ANNOTATION_FRAMES = 20\n",
    "    output_annote = None\n",
    "    f = 0\n",
    "    while cap.isOpened() and f < ANNOTATION_FRAMES:\n",
    "        f += 1\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output_annote is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output_annote = cv.VideoWriter('manual.mp4', -1, 2, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0]\n",
    "\n",
    "        faces = faces.numpy()\n",
    "\n",
    "        frame_faces = []\n",
    "\n",
    "        intact = frame.copy()\n",
    "\n",
    "        for box in faces.boxes:\n",
    "            tmp = intact.copy()\n",
    "            \n",
    "            xyxy = box.xyxy[0]\n",
    "            xyxy = [round(xy) for xy in xyxy]\n",
    "\n",
    "            tmp = cv.rectangle(tmp, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "\n",
    "            cv.imshow('', tmp)\n",
    "            cv.waitKey(0)\n",
    "            cv.destroyAllWindows()\n",
    "\n",
    "            # Annoto manualmente ogni bounding box nell'immagine con la vera identità (Davide,Stefano,Gabriele,Francesco o Unknown)\n",
    "\n",
    "            frame_face = input()\n",
    "            frame_faces.append(frame_face)\n",
    "\n",
    "            frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "            frame = cv.putText(frame, frame_face, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "        annotation.append(frame_faces)\n",
    "        output_annote.write(frame)\n",
    "    cap.release()\n",
    "    output_annote.release()\n",
    "\n",
    "    annotatefile = open('annotateset.pkl','wb')\n",
    "    pickle.dump(annotation, annotatefile)\n",
    "    annotatefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "MAX_FRAME = 20\n",
    "i = 0\n",
    "output = None\n",
    "\n",
    "test_data = []\n",
    "\n",
    "while cap.isOpened() and i < MAX_FRAME:\n",
    "    i += 1\n",
    "    if True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output = cv.VideoWriter('deep_auto_test.mp4', -1, 1, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0].numpy()\n",
    "\n",
    "        frame_data = []\n",
    "\n",
    "        if len(faces) != 0:\n",
    "            boxes = faces.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                xyxy = box.xyxy[0]\n",
    "                xyxy = [round(xy) for xy in xyxy]\n",
    "                crop = np.array([frame[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]])\n",
    "                crop = resize_and_rescale(crop)\n",
    "\n",
    "                pred = model.predict(crop, verbose=False)\n",
    "                conf = np.max(pred)\n",
    "                class_ = classes_[np.argmax(pred)]\n",
    "\n",
    "                frame_data.append(class_)\n",
    "\n",
    "                frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "                frame = cv.putText(frame, class_, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "\n",
    "            test_data.append(frame_data)\n",
    "\n",
    "            output.write(frame)\n",
    "                \n",
    "cap.release()\n",
    "output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy del nostro modello: dal momento che il face detector è lo stesso, ci basterà usare come metro di misura la somiglianza delle classi assegnate a ogni volto in ogni frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_enc= [LabelEncoder().fit_transform(x) for x in annotation]\n",
    "test_data_enc = [LabelEncoder().fit_transform(x) for x in test_data]\n",
    "\n",
    "acc = 0\n",
    "\n",
    "for annotation_item, test_item in zip(annotation_enc, test_data_enc):\n",
    "    score = accuracy_score(annotation_item, test_item) \n",
    "    acc += score\n",
    "\n",
    "acc /= len(annotation_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29166666666666663\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo adesso ad analizzare un video (che non ha overlap con la porzione usata per la validazione) per testare i risultati effettivi del nostro modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "MAX_FRAME = 2000\n",
    "i = 0\n",
    "output = None\n",
    "\n",
    "test_data = []\n",
    "\n",
    "while cap.isOpened() and i < MAX_FRAME:\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output = cv.VideoWriter('deep_auto.mp4', -1, 15, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0].numpy()\n",
    "\n",
    "        frame_data = []\n",
    "\n",
    "        if len(faces) != 0:\n",
    "            boxes = faces.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                xyxy = box.xyxy[0]\n",
    "                xyxy = [round(xy) for xy in xyxy]\n",
    "                crop = np.array([frame[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]])\n",
    "                crop = resize_and_rescale(crop)\n",
    "\n",
    "                pred = model.predict(crop, verbose=False)\n",
    "                conf = np.max(pred)\n",
    "                class_ = classes_[np.argmax(pred)]\n",
    "\n",
    "                frame_data.append(class_)\n",
    "\n",
    "                frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "                frame = cv.putText(frame, class_, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "\n",
    "            test_data.append(frame_data)\n",
    "\n",
    "            output.write(frame)\n",
    "                \n",
    "cap.release()\n",
    "output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misure di accuracy e considerazioni finali\n",
    "\n",
    "Sono stati testati 7 diversi modelli, con strutture e iperparametri più o meno diversi. Tuttavia i tentativi non hanno portato a risultati con accuracy soddisfacente. Potrebbe essere quindi ragionevole, a valle di tutto il lavoro svolto, dare molto più peso alla fase di raccolta, pulizia e selezione eventuale dei dati, cercando di catturare i volti scelti in condizioni più varie e tenendo conto di fattori come la presenza o meno degli occhiali, dal momento che il modello sembra comportarsi bene nel momento in cui sono stati fatti test di classificazioni usando le immagini di test del dataset di partenza, ma meno bene nel momento in cui le condizioni sono cambiate ed è stato usato un video con un setup leggermente diverso.\n",
    "\n",
    "Dal momento che ha dimostrato un'accuracy maggiore, andremo ad utilizzare il modello muzio_3.keras\n",
    "\n",
    "A seguire, la precisione ottenuta e la summary di ogni modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Model   |   Accuracy   |\n",
    "|-----------|--------------|\n",
    "|  Muzio 0  |      1%      |\n",
    "|  Muzio 1  |      23%     |\n",
    "|  Muzio 2  |      29%     |\n",
    "|  Muzio 3  |      15%     |\n",
    "|  Muzio 4  |      7%      |\n",
    "|  Muzio 5  |      5%      |\n",
    "|  Muzio 6  |      14%     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 32, 32, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 16, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               2097280   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2117317 (8.08 MB)\n",
      "Trainable params: 2117317 (8.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ = load_model('muzio_0.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 32, 32, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 16, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 8, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               524416    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 581381 (2.22 MB)\n",
      "Trainable params: 581381 (2.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ = load_model('muzio_1.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 14, 14, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 6, 6, 128)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 683845 (2.61 MB)\n",
      "Trainable params: 683845 (2.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ = load_model('muzio_2.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 31, 31, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 14, 14, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 6, 6, 128)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 683845 (2.61 MB)\n",
      "Trainable params: 683845 (2.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ = load_model('muzio_3.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 60, 60, 64)        4864      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 30, 30, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 14, 14, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 6, 6, 128)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 706245 (2.69 MB)\n",
      "Trainable params: 706245 (2.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ = load_model('muzio_4.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 64)        1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 31, 31, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 10, 10, 128)       204928    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 5, 5, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               409728    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 654021 (2.49 MB)\n",
      "Trainable params: 654021 (2.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ = load_model('muzio_5.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 32, 32, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 16, 16, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 8, 8, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 81925     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 452741 (1.73 MB)\n",
      "Trainable params: 452741 (1.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ = load_model('muzio_6.keras')\n",
    "\n",
    "model_.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
