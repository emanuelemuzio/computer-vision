{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Riconoscimento tramite Deep Learning\n",
    "\n",
    "Requisiti:\n",
    "- Rifacimento assignment sul riconoscimento, questa volta utilizzando però una rete neurale profonda.\n",
    "\n",
    "Per questa esercitazioni, le scelte tecniche sono state:\n",
    "- Yolov8 per la face detection nelle immagini e video\n",
    "- VGG16 riaddestrata sulle nostre immagini (soltanto negli ultimi layer)\n",
    "\n",
    "Bonus:\n",
    "- Utilizzo di una label extra per i volti sconosciuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import cv2 as cv\n",
    "import pickle, h5py\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Rescaling, Resizing, RandomFlip, RandomRotation, Flatten, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import tensorflow as tf\n",
    "\n",
    "yolo = YOLO('yolov8n-face.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo step consiste nel far scorrere il nostro face detector lungo tutte le immagini già acquisite, fare data augmentation grazie ai layer messi a disposizione da tensorflow e salvare i crop rilevati grazie a Yolo.\n",
    "\n",
    "Per la data augmentation, in particolare, sono state applicate trasformazioni randomizzate:\n",
    "- flip orizzontale dell'immagine\n",
    "- leggera rotazione (fattore 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializziamo le immagini per fare training, test e validazione\n",
    "\n",
    "train = None\n",
    "test = None\n",
    "val = None\n",
    "\n",
    "Imgs_train = Imgs_test = Imgs_val = None\n",
    "Labels_train = Labels_test = Labels_val = None\n",
    "\n",
    "if os.path.exists('train.h5'):\n",
    "    trainfile = h5py.File(\"train.h5\", \"r+\")\n",
    "    Imgs_train = np.array(trainfile[\"/images\"]).astype(\"float32\")\n",
    "    Labels_train = np.array(trainfile[\"/meta\"]).astype(\"float32\")\n",
    "\n",
    "    testfile = h5py.File(\"test.h5\", \"r+\")\n",
    "    Imgs_test = np.array(testfile[\"/images\"]).astype(\"float32\")\n",
    "    Labels_test = np.array(testfile[\"/meta\"]).astype(\"float32\")\n",
    "\n",
    "    valfile = h5py.File(\"val.h5\", \"r+\")\n",
    "    Imgs_val = np.array(valfile[\"/images\"]).astype(\"float32\")\n",
    "    Labels_val = np.array(valfile[\"/meta\"]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per le dimensioni di train-test-validation, abbiamo suddiviso le immagini etichettate precedentemente ottenute con le seguenti percentuali, rispettivamente 60-20-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processo di face detection dal nostro dataset. le foto sono circa 24k, su queste viene poi fatta data augmentation per arrivare sui 42k foto totali (divise tra insieme di addestramento, test e validazione)\n",
    "\n",
    "# Facciamo il resize facendo attenzione a mantenere l'aspect ratio per non avere dei crop totalmente distorti\n",
    "# Il rescaling è stato fatto in modo da mantenere valori tra -1 e 1 piuttosto che tra 0 e 1 per lavorare meglio\n",
    "# con la relu nel nostro modello, focalizzandoci sui valori maggiori di 0 (come da definizione per la Relu d'altronde)\n",
    "\n",
    "resize_and_rescale = Sequential([\n",
    "        Resizing(224, 224, crop_to_aspect_ratio=True),\n",
    "        Rescaling(scale=1./127.5, offset=-1),\n",
    "])\n",
    "\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "if not os.path.exists('train.h5'):\n",
    "    root = 'train'\n",
    "    paths = []\n",
    "    labels = []\n",
    "    dirs = ['Davide', 'Gabriele', 'Stefano']\n",
    "\n",
    "    for dir_ in dirs:\n",
    "        tmp = glob.glob(f'{root}/{dir_}/*.jpg')\n",
    "        labels.extend(len(tmp) * [dir_])\n",
    "        paths.extend(tmp)\n",
    "\n",
    "    imgs = []\n",
    "    img_labels = []\n",
    "\n",
    "    # Sfruttiamo il modello pre allenato di yolo specifico per il rilevamento facciale, nello specifico\n",
    "    # la versione nano per snellire i tempi\n",
    "\n",
    "    for i in range(len(paths)):\n",
    "        path = paths[i]\n",
    "        img = cv.imread(path)\n",
    "        aug = data_augmentation(img).numpy()\n",
    "\n",
    "        result = yolo(img, verbose=False)[0].cpu().numpy()\n",
    "        result_aug = yolo(aug, verbose=False)[0].cpu().numpy()\n",
    "\n",
    "        if len(result) != 0 and len(result_aug) != 0:\n",
    "\n",
    "            boxes = result.boxes\n",
    "            aug_boxes = result_aug.boxes\n",
    "\n",
    "            conf = boxes.conf\n",
    "            aug_conf = aug_boxes.conf\n",
    "\n",
    "            argmax = np.argmax(conf)\n",
    "            aug_argmax = np.argmax(aug_conf)\n",
    "\n",
    "            box = boxes[argmax]\n",
    "            aug_box = aug_boxes[aug_argmax]\n",
    "\n",
    "            xyxy = box.xyxy[0]\n",
    "            xyxy = [round(xy) for xy in xyxy]\n",
    "            crop = img[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]\n",
    "\n",
    "            aug_xyxy = aug_box.xyxy[0]\n",
    "            aug_xyxy = [round(xy) for xy in aug_xyxy]\n",
    "            aug_crop = img[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]\n",
    "\n",
    "            # 1. data aug\n",
    "            # 2. yolo (vado a prendere il quadrato che ingloba sostanzialmente la box di yolo per non deformare la faccia)(oppure preserve aspect ratio di tensorflow.image)\n",
    "            # 3. crop\n",
    "            # prima ruotiamo l'immagine, poi passiamo yolo sull'immagine ruotata\n",
    "            # in questo modo ritagliamo anche un po' di sfondo\n",
    "            # valutare di ignorare francesco\n",
    "\n",
    "            crop = resize_and_rescale(crop).numpy()\n",
    "            aug_crop = resize_and_rescale(crop).numpy()\n",
    "\n",
    "            imgs.append(crop)\n",
    "            imgs.append(aug_crop)\n",
    "            img_labels.append(labels[i])\n",
    "            img_labels.append(labels[i])\n",
    "\n",
    "    for path_ in glob.glob(f'{root}/Unknown/*.jpg'):\n",
    "        img = cv.imread(path)\n",
    "        result = yolo(img, verbose=False)[0].cpu().numpy()\n",
    "\n",
    "        if len(result) != 0:\n",
    "\n",
    "            boxes = result.boxes\n",
    "\n",
    "            conf = boxes.conf\n",
    "\n",
    "            argmax = np.argmax(conf)\n",
    "\n",
    "            box = boxes[argmax]\n",
    "\n",
    "            xyxy = box.xyxy[0]\n",
    "            xyxy = [round(xy) for xy in xyxy]\n",
    "            crop = img[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]\n",
    "            crop = resize_and_rescale(crop).numpy()\n",
    "\n",
    "            imgs.append(crop)\n",
    "            img_labels.append('Unknown')\n",
    "\n",
    "    # Procediamo a dividere le nostre immagini etichettate in 3 insiemi per fare poi\n",
    "    # train, test e validation. Le dimensioni di questi saranno rispettivamente il\n",
    "    # 60%, 20% e 20% del totale delle immagini a disposizione.\n",
    "    # Salviamo infine tramite pickle il lavoro svolto per evitare sprechi di tempo futuri\n",
    "\n",
    "    Imgs_train, Imgs_test, Labels_train, Labels_test = train_test_split(imgs, img_labels, test_size=0.4)\n",
    "    Imgs_val, Imgs_test, Labels_val, Labels_test = train_test_split(Imgs_test, Labels_test, test_size=0.5)\n",
    "    \n",
    "    Imgs_train = np.asarray(Imgs_train)\n",
    "    Imgs_test = np.asarray(Imgs_test)\n",
    "    Imgs_val = np.asarray(Imgs_val)\n",
    "    \n",
    "    Labels_train = LabelEncoder().fit_transform(Labels_train)\n",
    "    Labels_test = LabelEncoder().fit_transform(Labels_test)\n",
    "    Labels_val = LabelEncoder().fit_transform(Labels_val)\n",
    "\n",
    "    file = h5py.File(\"train.h5\", \"w\")\n",
    "\n",
    "    train_dataset = file.create_dataset(\n",
    "        \"images\", np.shape(Imgs_train), h5py.h5t.IEEE_F32LE, data=Imgs_train\n",
    "    )\n",
    "    train_meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(Labels_train), h5py.h5t.STD_U8BE, data=Labels_train\n",
    "    )\n",
    "    file.close()  \n",
    "\n",
    "    file = h5py.File(\"test.h5\", \"w\")\n",
    "\n",
    "    train_dataset = file.create_dataset(\n",
    "        \"images\", np.shape(Imgs_test), h5py.h5t.IEEE_F32LE, data=Imgs_test\n",
    "    )\n",
    "    train_meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(Labels_test), h5py.h5t.STD_U8BE, data=Labels_test\n",
    "    )\n",
    "    file.close()  \n",
    "\n",
    "    file = h5py.File(\"val.h5\", \"w\")\n",
    "\n",
    "    train_dataset = file.create_dataset(\n",
    "        \"images\", np.shape(Imgs_val), h5py.h5t.IEEE_F32LE, data=Imgs_val\n",
    "    )\n",
    "    train_meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(Labels_val), h5py.h5t.STD_U8BE, data=Labels_val\n",
    "    )\n",
    "    file.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgShape = (224, 224, 3)\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sono stati effettuati diversi tentativi al fine di scegliere il modello migliore per il nostro compito, variando sia iperparametri che struttura della rete, avvalendosi anche in alcuni tentativi di regolarizzazione l2 dei pesi in quanto, durante le prime prove, il problema più evidente era un overfitting in fase di test, che si traduceva poi in accuracy estremamente basse in fase di test.\n",
    "\n",
    "Per evitare sprechi di tempo e risorse, è stato utile aggiungere alla rete un meccanismo di early stopping mediamente sensibile sull'accuracy, nel caso in cui si arrivasse ad una situazione piatta e non riuscisse più ad aumentare dopo alcune epoche. \n",
    "\n",
    "La parte più importante è stata sicuramente quella di usare i pesi salvati nei vari checkpoint per riprendere l'addestramento in più parti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727/727 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.9311\n",
      "Epoch 13: val_loss did not improve from 0.22521\n",
      "727/727 [==============================] - 1053s 1s/step - loss: 0.2334 - accuracy: 0.9311 - val_loss: 0.2473 - val_accuracy: 0.9064 - lr: 3.1623e-04\n"
     ]
    }
   ],
   "source": [
    "model_name = 'muzio.keras'\n",
    "h5model = 'muzio.h5'\n",
    "fine_tune = True\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "  vgg = VGG16(weights='imagenet', include_top=False, input_shape=imgShape)\n",
    "\n",
    "  for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "  x = Flatten()(vgg.output)\n",
    "  prediction = Dense(num_classes, activation='softmax')(x)\n",
    "  model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  reduce_lr = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "  \n",
    "  early_stop = EarlyStopping(monitor='accuracy', patience = 3, restore_best_weights = True)\n",
    "\n",
    "  checkpoint = ModelCheckpoint(filepath=h5model, verbose=1, save_best_only=True)\n",
    "\n",
    "  if fine_tune:\n",
    "    model.load_weights(h5model)\n",
    "\n",
    "  hist = model.fit(Imgs_train, \n",
    "                Labels_train, \n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                validation_data=(Imgs_val, Labels_val),\n",
    "                callbacks=[early_stop, reduce_lr, checkpoint]\n",
    "                )\n",
    "    \n",
    "  model.save('muzio.keras')\n",
    "\n",
    "  # tentativo per inizializzare i filtri: encoder-decoder, stacchiamo il decoder, aggiungiamo dei dense \n",
    "\n",
    "  # suggerimento: inserire layer di dropout in coppia al learning rate (osservare se effettivamente riusciamo a minimizzare la loss)\n",
    "  # se la loss sale e scende devo abbassare il learning rate\n",
    "  # altro layer dense intermedio pre classificazione? magari 128\n",
    "  # più layer conv2d\n",
    "\n",
    "  # metrics -> val_loss l'importante è che la loss non cresca, la val loss sale e scende perchè non incide su addestramento\n",
    "  # la train loss deve sempre scendere\n",
    "\n",
    "  # LR: 0.01, 0.001, 0.0001\n",
    "else:\n",
    "  model = load_model(model_name)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraiamo 20 frame e annotiamoli manualmente per fare model selection: questi 20 frame non andranno a coincidere ovviamente con la porzione del video usata per i test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = ['Davide', 'Gabriele', 'Stefano']\n",
    "\n",
    "annotation = None\n",
    "font = cv.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "if os.path.exists('annotateset.pkl'):\n",
    "    annotationfile = open('annotateset.pkl', 'rb')\n",
    "    annotation = pickle.load(annotationfile)\n",
    "    annotationfile.close()\n",
    "else:\n",
    "    annotation = []\n",
    "    cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "    ANNOTATION_FRAMES = 20\n",
    "    output_annote = None\n",
    "    f = 0\n",
    "    while cap.isOpened() and f < ANNOTATION_FRAMES:\n",
    "        f += 1\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output_annote is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output_annote = cv.VideoWriter('manual.mp4', -1, 2, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0]\n",
    "\n",
    "        faces = faces.numpy()\n",
    "\n",
    "        frame_faces = []\n",
    "\n",
    "        intact = frame.copy()\n",
    "\n",
    "        for box in faces.boxes:\n",
    "            tmp = intact.copy()\n",
    "            \n",
    "            xyxy = box.xyxy[0]\n",
    "            xyxy = [round(xy) for xy in xyxy]\n",
    "\n",
    "            tmp = cv.rectangle(tmp, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "\n",
    "            cv.imshow('', tmp)\n",
    "            cv.waitKey(0)\n",
    "            cv.destroyAllWindows()\n",
    "\n",
    "            # Annoto manualmente ogni bounding box nell'immagine con la vera identità (Davide,Stefano,Gabriele o Unknown)\n",
    "\n",
    "            frame_face = input()\n",
    "            frame_faces.append(frame_face)\n",
    "\n",
    "            frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "            frame = cv.putText(frame, frame_face, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "        annotation.append(frame_faces)\n",
    "        output_annote.write(frame)\n",
    "    cap.release()\n",
    "    output_annote.release()\n",
    "\n",
    "    annotatefile = open('annotateset.pkl','wb')\n",
    "    pickle.dump(annotation, annotatefile)\n",
    "    annotatefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "MAX_FRAME = 20\n",
    "i = 0\n",
    "output = None\n",
    "\n",
    "test_data = []\n",
    "\n",
    "while cap.isOpened() and i < MAX_FRAME:\n",
    "    i += 1\n",
    "    if True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output = cv.VideoWriter('deep_auto_test.mp4', -1, 1, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0].numpy()\n",
    "\n",
    "        frame_data = []\n",
    "\n",
    "        if len(faces) != 0:\n",
    "            boxes = faces.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                xyxy = box.xyxy[0]\n",
    "                xyxy = [round(xy) for xy in xyxy]\n",
    "                crop = np.array([frame[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]])\n",
    "                crop = resize_and_rescale(crop)\n",
    "\n",
    "                pred = model.predict(crop, verbose=False)\n",
    "                conf = np.max(pred)\n",
    "                class_ = classes_[np.argmax(pred)]\n",
    "                frame_data.append(class_)\n",
    "\n",
    "                frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "                frame = cv.putText(frame, class_, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "\n",
    "            test_data.append(frame_data)\n",
    "\n",
    "            output.write(frame)\n",
    "                \n",
    "cap.release()\n",
    "output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy del nostro modello: dal momento che il face detector è lo stesso, ci basterà usare come metro di misura la somiglianza delle classi assegnate a ogni volto in ogni frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_enc= [LabelEncoder().fit_transform(x) for x in annotation]\n",
    "test_data_enc = [LabelEncoder().fit_transform(x) for x in test_data]\n",
    "\n",
    "acc = 0\n",
    "\n",
    "for annotation_item, test_item in zip(annotation_enc, test_data_enc):\n",
    "    score = accuracy_score(annotation_item, test_item) \n",
    "    acc += score\n",
    "\n",
    "acc /= len(annotation_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15416666666666665\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo adesso ad analizzare un video (che non ha overlap con la porzione usata per la validazione) per testare i risultati effettivi del nostro modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "MAX_FRAME = 2000\n",
    "i = 0\n",
    "output = None\n",
    "\n",
    "test_data = []\n",
    "\n",
    "while cap.isOpened() and i < MAX_FRAME:\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output = cv.VideoWriter('deep_auto.mp4', -1, 15, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0].numpy()\n",
    "\n",
    "        frame_data = []\n",
    "\n",
    "        if len(faces) != 0:\n",
    "            boxes = faces.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                xyxy = box.xyxy[0]\n",
    "                xyxy = [round(xy) for xy in xyxy]\n",
    "                crop = np.array([frame[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]])\n",
    "                crop = resize_and_rescale(crop)\n",
    "\n",
    "                pred = model.predict(crop, verbose=False)\n",
    "                conf = np.max(pred)\n",
    "                class_ = classes_[np.argmax(pred)]\n",
    "\n",
    "                frame_data.append(class_)\n",
    "\n",
    "                frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "                frame = cv.putText(frame, class_, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "\n",
    "            test_data.append(frame_data)\n",
    "\n",
    "            output.write(frame)\n",
    "                \n",
    "cap.release()\n",
    "output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misure di accuracy e considerazioni finali\n",
    "\n",
    "Sono stati testati 7 diversi modelli, con strutture e iperparametri più o meno diversi. Tuttavia i tentativi non hanno portato a risultati con accuracy soddisfacente. Potrebbe essere quindi ragionevole, a valle di tutto il lavoro svolto, dare molto più peso alla fase di raccolta, pulizia e selezione eventuale dei dati, cercando di catturare i volti scelti in condizioni più varie e tenendo conto di fattori come la presenza o meno degli occhiali, dal momento che il modello sembra comportarsi bene nel momento in cui sono stati fatti test di classificazioni usando le immagini di test del dataset di partenza, ma meno bene nel momento in cui le condizioni sono cambiate ed è stato usato un video con un setup leggermente diverso.\n",
    "\n",
    "Dal momento che ha dimostrato un'accuracy maggiore, andremo ad utilizzare il modello muzio_3.keras\n",
    "\n",
    "A seguire, la precisione ottenuta e la summary di ogni modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Model   |   Accuracy   |\n",
    "|-----------|--------------|\n",
    "|  Muzio 0  |      1%      |\n",
    "|  Muzio 1  |      23%     |\n",
    "|  Muzio 2  |      29%     |\n",
    "|  Muzio 3  |      15%     |\n",
    "|  Muzio 4  |      7%      |\n",
    "|  Muzio 5  |      5%      |\n",
    "|  Muzio 6  |      14%     |\n",
    "|  Muzio 7  |      37%     |\n",
    "|  Muzio    |      15%     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_0.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_1.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_2.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_3.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_4.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_5.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_6.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_7.keras')\n",
    "\n",
    "model_.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
