{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Riconoscimento tramite Deep Learning\n",
    "\n",
    "Requisiti:\n",
    "- Rifacimento assignment sul riconoscimento, questa volta utilizzando però una rete neurale profonda.\n",
    "\n",
    "Per questa esercitazioni, le scelte tecniche sono state:\n",
    "- Yolov8 per la face detection nelle immagini e video\n",
    "- Classificazione tramite CNN profonda in Tensorflow\n",
    "\n",
    "Bonus:\n",
    "- Utilizzo di una label extra per i volti sconosciuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !dpkg -l libcudnn8*\n",
    "# !apt-get update && apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy ultralytics scikit-learn opencv-python-headless h5py tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'GPUOptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m yolo \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n-face.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m yolo \u001b[38;5;241m=\u001b[39m yolo\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m gpu_options \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPUOptions\u001b[49m(allow_growth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m session \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mInteractiveSession(config\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mConfigProto(gpu_options\u001b[38;5;241m=\u001b[39mgpu_options))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'GPUOptions'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import cv2 as cv\n",
    "import pickle, h5py\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Rescaling, Resizing, RandomFlip, RandomRotation, Flatten, Dense, Input, Conv2D, MaxPooling2D\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "from tensorflow.test import is_gpu_available, is_built_with_cuda\n",
    "import tensorflow as tf\n",
    "\n",
    "yolo = YOLO('yolov8n-face.pt')\n",
    "yolo = yolo.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2612/582987611.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(is_gpu_available())\n",
    "print(is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo step consiste nel far scorrere il nostro face detector lungo tutte le immagini già acquisite, fare data augmentation grazie ai layer messi a disposizione da tensorflow e salvare i crop rilevati grazie a Yolo.\n",
    "\n",
    "Per la data augmentation, in particolare, sono state applicate trasformazioni randomizzate:\n",
    "- flip orizzontale dell'immagine\n",
    "- leggera rotazione (fattore 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializziamo le immagini per fare training, test e validazione\n",
    "\n",
    "train = None\n",
    "test = None\n",
    "val = None\n",
    "\n",
    "Imgs_train = Imgs_test = Imgs_val = None\n",
    "Labels_train = Labels_test = Labels_val = None\n",
    "\n",
    "if os.path.exists('train.h5'):\n",
    "    trainfile = h5py.File(\"train.h5\", \"r+\")\n",
    "    Imgs_train = np.array(trainfile[\"/images\"]).astype(\"uint8\")\n",
    "    Labels_train = np.array(trainfile[\"/meta\"]).astype(\"uint8\")\n",
    "\n",
    "    testfile = h5py.File(\"test.h5\", \"r+\")\n",
    "    Imgs_test = np.array(testfile[\"/images\"]).astype(\"uint8\")\n",
    "    Labels_test = np.array(testfile[\"/meta\"]).astype(\"uint8\")\n",
    "\n",
    "    valfile = h5py.File(\"val.h5\", \"r+\")\n",
    "    Imgs_val = np.array(valfile[\"/images\"]).astype(\"uint8\")\n",
    "    Labels_val = np.array(valfile[\"/meta\"]).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per le dimensioni di train-test-validation, abbiamo suddiviso le immagini etichettate precedentemente ottenute con le seguenti percentuali, rispettivamente 60-20-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processo di face detection dal nostro dataset. le foto sono circa 24k, su queste viene poi fatta data augmentation per arrivare sui 42k foto totali (divise tra insieme di addestramento, test e validazione)\n",
    "\n",
    "# Facciamo il resize facendo attenzione a mantenere l'aspect ratio per non avere dei crop totalmente distorti\n",
    "# Il rescaling è stato fatto in modo da mantenere valori tra -1 e 1 piuttosto che tra 0 e 1 per lavorare meglio\n",
    "# con la relu nel nostro modello, focalizzandoci sui valori maggiori di 0 (come da definizione per la Relu d'altronde)\n",
    "\n",
    "resize_and_rescale = Sequential([\n",
    "        Resizing(224, 224, crop_to_aspect_ratio=True),\n",
    "        Rescaling(scale=1./127.5, offset=-1),\n",
    "])\n",
    "\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "if not os.path.exists('train.h5'):\n",
    "    root = 'train'\n",
    "    paths = []\n",
    "    labels = []\n",
    "    dirs = ['Davide', 'Gabriele', 'Stefano']\n",
    "\n",
    "    for dir_ in dirs:\n",
    "        tmp = glob.glob(f'{root}/{dir_}/*.jpg')\n",
    "        labels.extend(len(tmp) * [dir_])\n",
    "        paths.extend(tmp)\n",
    "\n",
    "    imgs = []\n",
    "    img_labels = []\n",
    "\n",
    "    # Sfruttiamo il modello pre allenato di yolo specifico per il rilevamento facciale, nello specifico\n",
    "    # la versione nano per snellire i tempi\n",
    "\n",
    "    for i in range(len(paths)):\n",
    "        path = paths[i]\n",
    "        img = cv.imread(path)\n",
    "        aug = data_augmentation(img).numpy()\n",
    "\n",
    "        result = yolo(img, verbose=False)[0].cpu().numpy()\n",
    "        result_aug = yolo(aug, verbose=False)[0].cpu().numpy()\n",
    "\n",
    "        if len(result) != 0 and len(result_aug) != 0:\n",
    "\n",
    "            boxes = result.boxes\n",
    "            aug_boxes = result_aug.boxes\n",
    "\n",
    "            conf = boxes.conf\n",
    "            aug_conf = aug_boxes.conf\n",
    "\n",
    "            argmax = np.argmax(conf)\n",
    "            aug_argmax = np.argmax(aug_conf)\n",
    "\n",
    "            box = boxes[argmax]\n",
    "            aug_box = aug_boxes[aug_argmax]\n",
    "\n",
    "            xyxy = box.xyxy[0]\n",
    "            xyxy = [round(xy) for xy in xyxy]\n",
    "            crop = img[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]\n",
    "\n",
    "            aug_xyxy = aug_box.xyxy[0]\n",
    "            aug_xyxy = [round(xy) for xy in aug_xyxy]\n",
    "            aug_crop = img[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]\n",
    "\n",
    "            # 1. data aug\n",
    "            # 2. yolo (vado a prendere il quadrato che ingloba sostanzialmente la box di yolo per non deformare la faccia)(oppure preserve aspect ratio di tensorflow.image)\n",
    "            # 3. crop\n",
    "            # prima ruotiamo l'immagine, poi passiamo yolo sull'immagine ruotata\n",
    "            # in questo modo ritagliamo anche un po' di sfondo\n",
    "            # valutare di ignorare francesco\n",
    "\n",
    "            crop = resize_and_rescale(crop).numpy()\n",
    "            aug_crop = resize_and_rescale(crop).numpy()\n",
    "\n",
    "            imgs.append(crop)\n",
    "            imgs.append(aug_crop)\n",
    "            img_labels.append(labels[i])\n",
    "            img_labels.append(labels[i])\n",
    "\n",
    "    for path_ in glob.glob(f'{root}/Unknown/*.jpg'):\n",
    "        img = cv.imread(path)\n",
    "        result = yolo(img, verbose=False)[0].cpu().numpy()\n",
    "\n",
    "        if len(result) != 0:\n",
    "\n",
    "            boxes = result.boxes\n",
    "\n",
    "            conf = boxes.conf\n",
    "\n",
    "            argmax = np.argmax(conf)\n",
    "\n",
    "            box = boxes[argmax]\n",
    "\n",
    "            xyxy = box.xyxy[0]\n",
    "            xyxy = [round(xy) for xy in xyxy]\n",
    "            crop = img[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]\n",
    "            crop = resize_and_rescale(crop).numpy()\n",
    "\n",
    "            imgs.append(crop)\n",
    "            img_labels.append('Unknown')\n",
    "\n",
    "    # Procediamo a dividere le nostre immagini etichettate in 3 insiemi per fare poi\n",
    "    # train, test e validation. Le dimensioni di questi saranno rispettivamente il\n",
    "    # 60%, 20% e 20% del totale delle immagini a disposizione.\n",
    "    # Salviamo infine tramite pickle il lavoro svolto per evitare sprechi di tempo futuri\n",
    "\n",
    "    Imgs_train, Imgs_test, Labels_train, Labels_test = train_test_split(imgs, img_labels, test_size=0.4)\n",
    "    Imgs_val, Imgs_test, Labels_val, Labels_test = train_test_split(Imgs_test, Labels_test, test_size=0.5)\n",
    "    \n",
    "    Imgs_train = np.asarray(Imgs_train)\n",
    "    Imgs_test = np.asarray(Imgs_test)\n",
    "    Imgs_val = np.asarray(Imgs_val)\n",
    "    \n",
    "    Labels_train = LabelEncoder().fit_transform(Labels_train)\n",
    "    Labels_test = LabelEncoder().fit_transform(Labels_test)\n",
    "    Labels_val = LabelEncoder().fit_transform(Labels_val)\n",
    "\n",
    "    file = h5py.File(\"train.h5\", \"w\")\n",
    "\n",
    "    train_dataset = file.create_dataset(\n",
    "        \"images\", np.shape(Imgs_train), h5py.h5t.STD_U8BE, data=Imgs_train\n",
    "    )\n",
    "    train_meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(Labels_train), h5py.h5t.STD_U8BE, data=Labels_train\n",
    "    )\n",
    "    file.close()  \n",
    "\n",
    "    file = h5py.File(\"test.h5\", \"w\")\n",
    "\n",
    "    train_dataset = file.create_dataset(\n",
    "        \"images\", np.shape(Imgs_test), h5py.h5t.STD_U8BE, data=Imgs_test\n",
    "    )\n",
    "    train_meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(Labels_test), h5py.h5t.STD_U8BE, data=Labels_test\n",
    "    )\n",
    "    file.close()  \n",
    "\n",
    "    file = h5py.File(\"val.h5\", \"w\")\n",
    "\n",
    "    train_dataset = file.create_dataset(\n",
    "        \"images\", np.shape(Imgs_val), h5py.h5t.STD_U8BE, data=Imgs_val\n",
    "    )\n",
    "    train_meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(Labels_val), h5py.h5t.STD_U8BE, data=Labels_val\n",
    "    )\n",
    "    file.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgShape = (224, 224, 3)\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sono stati effettuati diversi tentativi al fine di scegliere il modello migliore per il nostro compito, variando sia iperparametri che struttura della rete, avvalendosi anche in alcuni tentativi di regolarizzazione l2 dei pesi in quanto, durante le prime prove, il problema più evidente era un overfitting in fase di test, che si traduceva poi in accuracy estremamente basse in fase di test.\n",
    "\n",
    "Per evitare sprechi di tempo e risorse, è stato utile aggiungere alla rete un meccanismo di early stopping mediamente sensibile sull'accuracy, nel caso in cui si arrivasse ad una situazione piatta e non riuscisse più ad aumentare dopo alcune epoche. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensor_test \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImgs_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "tensor_test = tf.convert_to_tensor(Imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m generate_new_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fine_tune \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImgs_train\u001b[49m\u001b[43m)\u001b[49m, tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(Labels_train)))\n\u001b[1;32m      6\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(Imgs_test), tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(Labels_test)))\n\u001b[1;32m      7\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(Imgs_val), tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(Labels_val)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "model_name = 'muzio.keras'\n",
    "h5model = 'muzio.h5'\n",
    "generate_new_model = True\n",
    "fine_tune = False\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(Imgs_train), tf.convert_to_tensor(Labels_train)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(Imgs_test), tf.convert_to_tensor(Labels_test)))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(Imgs_val), tf.convert_to_tensor(Labels_val)))\n",
    "\n",
    "if os.path.exists(model_name) and not generate_new_model:\n",
    "  model = load_model(model_name)\n",
    "else:\n",
    "    _input = Input(imgShape) \n",
    "    \n",
    "    conv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(_input)\n",
    "    conv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    pool1  = MaxPooling2D((2, 2))(conv2)\n",
    "    \n",
    "    conv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "    conv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "    pool2  = MaxPooling2D((2, 2))(conv4)\n",
    "    \n",
    "    conv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "    conv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "    conv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "    pool3  = MaxPooling2D((2, 2))(conv7)\n",
    "    \n",
    "    conv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "    conv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "    conv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "    pool4  = MaxPooling2D((2, 2))(conv10)\n",
    "    \n",
    "    conv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "    conv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\n",
    "    conv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\n",
    "    pool5  = MaxPooling2D((2, 2))(conv13)\n",
    "    \n",
    "    flat   = Flatten()(pool5)\n",
    "    dense1 = Dense(4096, activation=\"relu\")(flat)\n",
    "    dense2 = Dense(4096, activation=\"relu\")(dense1)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(dense2)\n",
    "    \n",
    "    model  = Model(inputs=_input, outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "  \n",
    "    early_stop = EarlyStopping(monitor='accuracy', patience = 3, restore_best_weights = True)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=h5model, verbose=1, save_best_only=True)\n",
    "\n",
    "    hist = model.fit(train_dataset, \n",
    "                  epochs=18,\n",
    "                  batch_size=8,\n",
    "                  validation_data=val_dataset,\n",
    "                  callbacks=[early_stop, reduce_lr, checkpoint]\n",
    "                  )\n",
    "  # tentativo per inizializzare i filtri: encoder-decoder, stacchiamo il decoder, aggiungiamo dei dense \n",
    "\n",
    "  # suggerimento: inserire layer di dropout in coppia al learning rate (osservare se effettivamente riusciamo a minimizzare la loss)\n",
    "  # se la loss sale e scende devo abbassare il learning rate\n",
    "  # altro layer dense intermedio pre classificazione? magari 128\n",
    "  # più layer conv2d\n",
    "\n",
    "  # metrics -> val_loss l'importante è che la loss non cresca, la val loss sale e scende perchè non incide su addestramento\n",
    "  # la train loss deve sempre scendere\n",
    "\n",
    "  # LR: 0.01, 0.001, 0.0001\n",
    "  \n",
    "    model.save('muzio.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraiamo 20 frame e annotiamoli manualmente per fare model selection: questi 20 frame non andranno a coincidere ovviamente con la porzione del video usata per i test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = ['Davide', 'Gabriele', 'Stefano']\n",
    "\n",
    "annotation = None\n",
    "font = cv.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "if os.path.exists('annotateset.pkl'):\n",
    "    annotationfile = open('annotateset.pkl', 'rb')\n",
    "    annotation = pickle.load(annotationfile)\n",
    "    annotationfile.close()\n",
    "else:\n",
    "    annotation = []\n",
    "    cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "    ANNOTATION_FRAMES = 20\n",
    "    output_annote = None\n",
    "    f = 0\n",
    "    while cap.isOpened() and f < ANNOTATION_FRAMES:\n",
    "        f += 1\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output_annote is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output_annote = cv.VideoWriter('manual.mp4', -1, 2, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0]\n",
    "\n",
    "        faces = faces.numpy()\n",
    "\n",
    "        frame_faces = []\n",
    "\n",
    "        intact = frame.copy()\n",
    "\n",
    "        for box in faces.boxes:\n",
    "            tmp = intact.copy()\n",
    "            \n",
    "            xyxy = box.xyxy[0]\n",
    "            xyxy = [round(xy) for xy in xyxy]\n",
    "\n",
    "            tmp = cv.rectangle(tmp, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "\n",
    "            cv.imshow('', tmp)\n",
    "            cv.waitKey(0)\n",
    "            cv.destroyAllWindows()\n",
    "\n",
    "            # Annoto manualmente ogni bounding box nell'immagine con la vera identità (Davide,Stefano,Gabriele o Unknown)\n",
    "\n",
    "            frame_face = input()\n",
    "            frame_faces.append(frame_face)\n",
    "\n",
    "            frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "            frame = cv.putText(frame, frame_face, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "        annotation.append(frame_faces)\n",
    "        output_annote.write(frame)\n",
    "    cap.release()\n",
    "    output_annote.release()\n",
    "\n",
    "    annotatefile = open('annotateset.pkl','wb')\n",
    "    pickle.dump(annotation, annotatefile)\n",
    "    annotatefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "MAX_FRAME = 20\n",
    "i = 0\n",
    "output = None\n",
    "\n",
    "test_data = []\n",
    "\n",
    "while cap.isOpened() and i < MAX_FRAME:\n",
    "    i += 1\n",
    "    if True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output = cv.VideoWriter('deep_auto_test.mp4', -1, 1, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0].numpy()\n",
    "\n",
    "        frame_data = []\n",
    "\n",
    "        if len(faces) != 0:\n",
    "            boxes = faces.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                xyxy = box.xyxy[0]\n",
    "                xyxy = [round(xy) for xy in xyxy]\n",
    "                crop = np.array([frame[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]])\n",
    "                crop = resize_and_rescale(crop)\n",
    "\n",
    "                pred = model.predict(crop, verbose=False)\n",
    "                print(pred)\n",
    "                conf = np.max(pred)\n",
    "                class_ = classes_[np.argmax(pred)]\n",
    "                frame_data.append(class_)\n",
    "\n",
    "                frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "                frame = cv.putText(frame, class_, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "\n",
    "            test_data.append(frame_data)\n",
    "\n",
    "            output.write(frame)\n",
    "                \n",
    "cap.release()\n",
    "output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy del nostro modello: dal momento che il face detector è lo stesso, ci basterà usare come metro di misura la somiglianza delle classi assegnate a ogni volto in ogni frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_enc= [LabelEncoder().fit_transform(x) for x in annotation]\n",
    "test_data_enc = [LabelEncoder().fit_transform(x) for x in test_data]\n",
    "\n",
    "acc = 0\n",
    "\n",
    "for annotation_item, test_item in zip(annotation_enc, test_data_enc):\n",
    "    score = accuracy_score(annotation_item, test_item) \n",
    "    acc += score\n",
    "\n",
    "acc /= len(annotation_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo adesso ad analizzare un video (che non ha overlap con la porzione usata per la validazione) per testare i risultati effettivi del nostro modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('Video finale senza riconoscimento.mp4')\n",
    "MAX_FRAME = 2000\n",
    "i = 0\n",
    "output = None\n",
    "\n",
    "test_data = []\n",
    "\n",
    "while cap.isOpened() and i < MAX_FRAME:\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        if output is None:\n",
    "            height, width, channels = frame.shape\n",
    "            size = (width,height)\n",
    "            output = cv.VideoWriter('deep_auto.mp4', -1, 15, size)\n",
    "\n",
    "        faces = yolo(frame, verbose=False)[0].numpy()\n",
    "\n",
    "        frame_data = []\n",
    "\n",
    "        if len(faces) != 0:\n",
    "            boxes = faces.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                xyxy = box.xyxy[0]\n",
    "                xyxy = [round(xy) for xy in xyxy]\n",
    "                crop = np.array([frame[xyxy[1] : xyxy[3], xyxy[0] : xyxy[2]]])\n",
    "                crop = resize_and_rescale(crop)\n",
    "\n",
    "                pred = model.predict(crop, verbose=False)\n",
    "                conf = np.max(pred)\n",
    "                class_ = classes_[np.argmax(pred)]\n",
    "\n",
    "                frame_data.append(class_)\n",
    "\n",
    "                frame = cv.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 0, 255), 1)\n",
    "                frame = cv.putText(frame, class_, (xyxy[0], xyxy[1]), font, 1, (0, 0, 255), 1, cv.LINE_AA, False)\n",
    "\n",
    "            test_data.append(frame_data)\n",
    "\n",
    "            output.write(frame)\n",
    "                \n",
    "cap.release()\n",
    "output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misure di accuracy e considerazioni finali\n",
    "\n",
    "Sono stati testati 7 diversi modelli, con strutture e iperparametri più o meno diversi. Tuttavia i tentativi non hanno portato a risultati con accuracy soddisfacente. Potrebbe essere quindi ragionevole, a valle di tutto il lavoro svolto, dare molto più peso alla fase di raccolta, pulizia e selezione eventuale dei dati, cercando di catturare i volti scelti in condizioni più varie e tenendo conto di fattori come la presenza o meno degli occhiali, dal momento che il modello sembra comportarsi bene nel momento in cui sono stati fatti test di classificazioni usando le immagini di test del dataset di partenza, ma meno bene nel momento in cui le condizioni sono cambiate ed è stato usato un video con un setup leggermente diverso.\n",
    "\n",
    "Dal momento che ha dimostrato un'accuracy maggiore, andremo ad utilizzare il modello muzio_3.keras\n",
    "\n",
    "A seguire, la precisione ottenuta e la summary di ogni modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Model   |   Accuracy   |\n",
    "|-----------|--------------|\n",
    "|  Muzio 0  |      1%      |\n",
    "|  Muzio 1  |      23%     |\n",
    "|  Muzio 2  |      29%     |\n",
    "|  Muzio 3  |      15%     |\n",
    "|  Muzio 4  |      7%      |\n",
    "|  Muzio 5  |      5%      |\n",
    "|  Muzio 6  |      14%     |\n",
    "|  Muzio 7  |      37%     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_0.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_1.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_2.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_3.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_4.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_5.keras')\n",
    "\n",
    "model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = load_model('muzio_6.keras')\n",
    "\n",
    "model_.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
